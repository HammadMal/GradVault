{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Accelerated Computing - Assignment 1\n",
    "Muhammad Meesum Ali Qazalbash (mq06861)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
    "%load_ext nvcc_plugin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Write CUDA code to initialize a random array of 1000000 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <curand.h>\n",
    "#include <curand_kernel.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "#define N 1000000\n",
    "\n",
    "inline cudaError_t checkCudaErr(cudaError_t err, const char *msg) {\n",
    "    if (err != cudaSuccess) {\n",
    "        fprintf(stderr, \"CUDA Runtime error at %s: %s\\n\", msg,\n",
    "                cudaGetErrorString(err));\n",
    "    }\n",
    "    return err;\n",
    "}\n",
    "\n",
    "__global__ void init_rand(curandState *state, unsigned long seed) {\n",
    "    int id = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    curand_init(seed, id, 0, &state[id]);\n",
    "}\n",
    "\n",
    "__global__ void generate_rand(curandState *state, float *rand) {\n",
    "    int id   = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    rand[id] = curand_uniform(&state[id]);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    float       *rande, *d_rand, cpu_time_used, gpu_time_used, memcpy_time_used;\n",
    "    int          size = N * sizeof(float), state_size = N * sizeof(curandState);\n",
    "    curandState *state, *d_state;\n",
    "    clock_t      start, end;\n",
    "    cudaEvent_t  start_gpu, stop_gpu;\n",
    "\n",
    "    rande = (float *)malloc(size);\n",
    "    state = (curandState *)malloc(state_size);\n",
    "\n",
    "    checkCudaErr(cudaMalloc((void **)&d_rand, size), \"cudaMalloc\");\n",
    "    checkCudaErr(cudaMalloc((void **)&d_state, state_size), \"cudaMalloc\");\n",
    "\n",
    "    start = clock();\n",
    "    for (int i = 0; i < N; i++) rande[i] = (float)rand() / (float)RAND_MAX;\n",
    "    end           = clock();\n",
    "    cpu_time_used = ((float)(end - start)) / CLOCKS_PER_SEC;\n",
    "    printf(\"CPU time used:    %10f seconds to generate random numbers on CPU \\n\",\n",
    "           cpu_time_used);\n",
    "            \n",
    "    checkCudaErr(cudaEventCreate(&start_gpu), \"cudaEventCreate\");\n",
    "    checkCudaErr(cudaEventCreate(&stop_gpu), \"cudaEventCreate\");\n",
    "    checkCudaErr(cudaEventRecord(start_gpu, 0), \"cudaEventRecord\");\n",
    "\n",
    "    init_rand<<<N / 256, 256>>>(d_state, time(NULL));\n",
    "    generate_rand<<<N / 256, 256>>>(d_state, d_rand);\n",
    "\n",
    "    checkCudaErr(cudaEventRecord(stop_gpu, 0), \"cudaEventRecord\");\n",
    "    checkCudaErr(cudaEventSynchronize(stop_gpu), \"cudaEventSynchronize\");\n",
    "    checkCudaErr(cudaEventElapsedTime(&gpu_time_used, start_gpu, stop_gpu), \"cudaEventElapsedTime\");\n",
    "\n",
    "    printf(\"GPU time used:    %10f seconds to generate random numbers on GPU \\n\", gpu_time_used / 1000.0f);\n",
    "            \n",
    "    start = clock();\n",
    "\n",
    "    checkCudaErr(cudaMemcpy(rande, d_rand, size, cudaMemcpyDeviceToHost),\n",
    "                 \"cudaMemcpy\");\n",
    "     end = clock();\n",
    "     memcpy_time_used = ((float)(end - start)) / CLOCKS_PER_SEC;\n",
    "     printf(\"MEMCPY time used: %10f seconds to copy random numbers from GPU to CPU\\n\", memcpy_time_used / 1000.0f);\n",
    "\n",
    "    checkCudaErr(cudaFree(d_rand), \"cudaFree\");\n",
    "    checkCudaErr(cudaFree(d_state), \"cudaFree\");\n",
    "\n",
    "    free(rande);\n",
    "    free(state);\n",
    "\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Write CUDA code to calculate the sum of 1000 elements array and output the sum. Calculate the total time for doing the calculation on the CPU as well as on the GPU. Use an appropriate execution configuration when you launch your kernel. Share the following information from your solution.\n",
    "## Explaination\n",
    "The parallel logic for reduction is used to calculate the sum of the array. The logic is as follows:\n",
    "1. The array is divided into blocks.\n",
    "2. Each block is then reduced to a single element using the parallel logic.\n",
    "3. The final sum is calculated by adding the reduced elements of each block.\n",
    "## Reference\n",
    "[GitHub reference](https://github.com/CoffeeBeforeArch/cuda_programming/blob/master/03_sum_reduction/bank_conflicts/sumReduction.cu)\n",
    "## Plots\n",
    "They can be found by running the code in the one after this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "#include <curand.h>\n",
    "#include <curand_kernel.h>\n",
    "#include <stdio.h>\n",
    "#include <time.h>\n",
    "\n",
    "#define SHARED_MEMORY_SIZE 250\n",
    "\n",
    "/**\n",
    " * @brief Checks for CUDA errors and prints the error message\n",
    " *\n",
    " * @param err\n",
    " * @param msg\n",
    " * @return cudaError_t\n",
    " */\n",
    "inline cudaError_t checkCudaErr(cudaError_t err, const char *msg) {\n",
    "    if (err != cudaSuccess) {\n",
    "        fprintf(stderr, \"CUDA Runtime error at %s: %s\\n\", msg,\n",
    "                cudaGetErrorString(err));\n",
    "    }\n",
    "    return err;\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief Calculates the sum of an array using a reduction algorithm\n",
    " *\n",
    " * @param vector The input vector\n",
    " * @param reduced_vector The output vector\n",
    " */\n",
    "__global__ void sumThroughReduction(const int *vector, int *reduced_vector) {\n",
    "    __shared__ int partial_sum[SHARED_MEMORY_SIZE];  // Shared memory\n",
    "    int id = blockIdx.x * blockDim.x + threadIdx.x;  // Global thread ID\n",
    "    partial_sum[threadIdx.x] = vector[id];  // Load data into shared memory\n",
    "\n",
    "    __syncthreads();  // Wait for all threads to load data into shared memory\n",
    "                      // before starting the reduction\n",
    "\n",
    "    // Reduction in shared memory (sequential addressing)\n",
    "    int index;\n",
    "    for (int s = 1; s < blockDim.x; s *= 2) {\n",
    "        // Each thread does work unless the index goes off the block (s is the\n",
    "        // stride)\n",
    "        index = 2 * s * threadIdx.x;  // Index of the element to be added\n",
    "\n",
    "        // Add the element at index to the element at index + s\n",
    "        if (index < blockDim.x) partial_sum[index] += partial_sum[index + s];\n",
    "\n",
    "        __syncthreads();  // Wait for all threads to finish before starting the\n",
    "                          // next iteration\n",
    "    }\n",
    "\n",
    "    // Write the result for this block to the output vector at the correct index\n",
    "    if (threadIdx.x == 0) reduced_vector[blockIdx.x] = partial_sum[0];\n",
    "}\n",
    "\n",
    "/**\n",
    " * @brief The main function of the program\n",
    " *\n",
    " * @return int\n",
    " */\n",
    "int main(void) {\n",
    "    float       cpu_time_used, gpu_time_used;\n",
    "    clock_t     start, end;\n",
    "    cudaEvent_t start_gpu, stop_gpu;\n",
    "\n",
    "    int ARRAY_SIZES[3] = {10000, 100000, 1000000};\n",
    "\n",
    "    FILE *fp = fopen(\"output.csv\", \"w\");\n",
    "    fprintf(fp, \"Dataset size,Grid size,Thread size,CPU time,GPU time\\n\");\n",
    "\n",
    "    for (int ARRAY_SIZE : ARRAY_SIZES) {\n",
    "        // Size of the array in bytes\n",
    "        int size = ARRAY_SIZE * sizeof(int);\n",
    "\n",
    "        // Host data (initial and final) (CPU)\n",
    "        int *initial_host_data = (int *)malloc(size);\n",
    "        int *final_host_data   = (int *)malloc(size);\n",
    "\n",
    "        int i = 0;\n",
    "\n",
    "        // Initialize the host data with some values (1,2,3,...) and set the\n",
    "        // final host data to 0\n",
    "        for (; i < ARRAY_SIZE; i++) {\n",
    "            initial_host_data[i] = i + 1;\n",
    "            final_host_data[i]   = 0;\n",
    "        }\n",
    "\n",
    "        start   = clock();\n",
    "        int sum = 0;\n",
    "        // Calculate the sum on the host (CPU)\n",
    "        for (i = 0; i < ARRAY_SIZE;) sum += initial_host_data[i++];\n",
    "        end           = clock();\n",
    "        cpu_time_used = ((float)(end - start)) / CLOCKS_PER_SEC;\n",
    "        printf(\"CPU time used: %f seconds for sum = %d\\n\", cpu_time_used, sum);\n",
    "\n",
    "        int GRIDS[] = {ARRAY_SIZE / 250, ARRAY_SIZE / 500, ARRAY_SIZE / 1000};\n",
    "        int THREADS = 250;\n",
    "\n",
    "        // Device data (initial and final) (GPU)\n",
    "        int *initial_device_data, *final_device_data;\n",
    "        checkCudaErr(cudaMalloc((void **)&initial_device_data, size),\n",
    "                     \"cudaMalloc\");\n",
    "        checkCudaErr(cudaMalloc((void **)&final_device_data, size),\n",
    "                     \"cudaMalloc\");\n",
    "\n",
    "        for (int i = 0; i < 3; i++) {\n",
    "            // Copy data from host to device (CPU -> GPU)\n",
    "            checkCudaErr(cudaMemcpy(initial_device_data, initial_host_data,\n",
    "                                    size, cudaMemcpyHostToDevice),\n",
    "                         \"cudaMemcpy\");\n",
    "            checkCudaErr(cudaMemcpy(final_device_data, final_host_data, size,\n",
    "                                    cudaMemcpyHostToDevice),\n",
    "                         \"cudaMemcpy\");\n",
    "\n",
    "            // Create events to measure the time taken by the GPU\n",
    "            checkCudaErr(cudaEventCreate(&start_gpu), \"cudaEventCreate\");\n",
    "            checkCudaErr(cudaEventCreate(&stop_gpu), \"cudaEventCreate\");\n",
    "            checkCudaErr(cudaEventRecord(start_gpu, 0), \"cudaEventRecord\");\n",
    "\n",
    "            // Launch the kernel\n",
    "            sumThroughReduction<<<GRIDS[i], THREADS>>>(initial_device_data,\n",
    "                                                       final_device_data);\n",
    "\n",
    "            // Launch the kernel again to reduce the final\n",
    "            // data to a single value (the sum)\n",
    "            sumThroughReduction<<<1, THREADS>>>(final_device_data,\n",
    "                                                final_device_data);\n",
    "\n",
    "            // Copy data from device to host (GPU -> CPU)\n",
    "            checkCudaErr(cudaMemcpy(final_host_data, final_device_data, size,\n",
    "                                    cudaMemcpyDeviceToHost),\n",
    "                         \"cudaMemcpy\");\n",
    "\n",
    "            checkCudaErr(cudaEventRecord(stop_gpu, 0), \"cudaEventRecord\");\n",
    "            checkCudaErr(cudaEventSynchronize(stop_gpu),\n",
    "                         \"cudaEventSynchronize\");\n",
    "            checkCudaErr(\n",
    "                cudaEventElapsedTime(&gpu_time_used, start_gpu, stop_gpu),\n",
    "                \"cudaEventElapsedTime\");\n",
    "\n",
    "            // Print the result\n",
    "            printf(\"Grid size: %d, Thread size: %d\\n\", GRIDS[i], THREADS);\n",
    "            printf(\"GPU time used: %f seconds for sum = %d\\n\",\n",
    "                   gpu_time_used / 1000.0f, final_host_data[0]);\n",
    "\n",
    "            fprintf(fp, \"%d,%d,%d,%f,%f\\n\", ARRAY_SIZE, GRIDS[i], THREADS,\n",
    "                    cpu_time_used, gpu_time_used / 1000.0f);\n",
    "        }\n",
    "\n",
    "        // Free the memory\n",
    "        free(initial_host_data);\n",
    "        free(final_host_data);\n",
    "        cudaFree(initial_device_data);\n",
    "        cudaFree(final_device_data);\n",
    "    }\n",
    "\n",
    "    fclose(fp);\n",
    "\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('output.csv', delimiter=',', skip_header=1)\n",
    "data = data[data[:, 0].argsort()]\n",
    "dataset_sizes = np.unique(data[:, 0])\n",
    "\n",
    "for dataset_size in dataset_sizes:\n",
    "\n",
    "    data_subset = data[data[:, 0] == dataset_size]\n",
    "    grid_size = data_subset[:, 1]\n",
    "    thread_size = data_subset[:, 2]\n",
    "    gpu_time = data_subset[:, 4]\n",
    "    cpu_time = data_subset[:, 3]\n",
    "\n",
    "    plt.axhline(y=cpu_time[0], color='r', linestyle='-')\n",
    "    plt.text(0, cpu_time[0], 'CPU time: ' + str(cpu_time[0]))\n",
    "    plt.bar(range(3), gpu_time, width=0.5, align='center')\n",
    "    plt.title('Dataset size: ' + str(dataset_size))\n",
    "    plt.xlabel('<<<Grid size, Thread size>>>')\n",
    "    plt.ylabel('GPU time')\n",
    "    plt.xticks(range(3), zip(grid_size, thread_size))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
